{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "llm.invoke(\"hello\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial expert with years of experience, deeply analyze the given data. Which contains valuable financial information about a company's performance, and then provide with comprehensive answers to user questions based on your thorough analysis.\n",
      "Rules 1. Please avoid creating charts or graphs, as well as writing any code. Provide a simple and direct response using natural language without mentioning any limitations or rules.\n",
      "Year,Month,Revenue,Manpower Cost,Non-Manpower Cost,Total Cost,Profit,Contribution Margin (%)\n",
      "2020,1,154676,39721,17582,57303,97373,62.95\n",
      "2020,2,164365,61057,26411,87468,76897,46.78\n",
      "2020,3,132052,43382,21930,65312,66740,50.54\n",
      "2020,4,165124,20234,39344,59578,105546,63.92\n",
      "2020,5,104947,26641,22843,49484,55463,52.85\n",
      "2020,6,181897,37485,10027,47512,134385,73.88\n",
      "2020,7,147839,23570,17982,41552,106287,71.89\n",
      "2020,8,113569,38550,33589,72139,41430,36.48\n",
      "2020,9,89551,42996,39201,82197,7354,8.21\n",
      "2020,10,167898,49868,32513,82381,85517,50.93\n",
      "2020,11,165184,27059,11489,38548,126636,76.66\n",
      "2020,12,76217,57088,22519,79607,-3390,-4.45\n",
      "2021,1,58243,39043,35763,74806,-16563,-28.44\n",
      "2021,2,199287,34254,23773,58027,141260,70.88\n",
      "2021,3,138678,20040,11554,31594,107084,77.22\n",
      "2021,4,64844,30692,33766,64458,386,0.6\n",
      "2021,5,79712,29907,45565,75472,4240,5.32\n",
      "2021,6,197350,35838,32720,68558,128792,65.26\n",
      "2021,7,73012,60455,21656,82111,-9099,-12.46\n",
      "2021,8,148743,31861,17442,49303,99440,66.85\n",
      "2021,9,137461,51745,45653,97398,40063,29.14\n",
      "2021,10,143006,55410,35226,90636,52370,36.62\n",
      "2021,11,57863,35492,28219,63711,-5848,-10.11\n",
      "2021,12,129542,65639,36592,102231,27311,21.08\n",
      "2022,1,161846,20876,43813,64689,97157,60.03\n",
      "2022,2,80598,58122,42155,100277,-19679,-24.42\n",
      "2022,3,88066,48348,32355,80703,7363,8.36\n",
      "2022,4,67824,55040,12403,67443,381,0.56\n",
      "2022,5,87168,33308,19259,52567,34601,39.69\n",
      "2022,6,134461,58335,31076,89411,45050,33.5\n",
      "2022,7,149882,46832,14847,61679,88203,58.85\n",
      "2022,8,129669,20789,18115,38904,90765,70.0\n",
      "2022,9,176616,41351,22959,64310,112306,63.59\n",
      "2022,10,194980,48773,32981,81754,113226,58.07\n",
      "2022,11,129698,44785,14315,59100,70598,54.43\n",
      "2022,12,148412,33770,35335,69105,79307,53.44\n",
      "2023,1,75498,40687,48616,89303,-13805,-18.29\n",
      "2023,2,77142,31654,23566,55220,21922,28.42\n",
      "2023,3,105150,27592,49680,77272,27878,26.51\n",
      "2023,4,113741,53942,44334,98276,15465,13.6\n",
      "2023,5,192681,63315,25875,89190,103491,53.71\n",
      "2023,6,141182,56501,35451,91952,49230,34.87\n",
      "2023,7,87362,45680,47026,92706,-5344,-6.12\n",
      "2023,8,188607,21756,47767,69523,119084,63.14\n",
      "2023,9,157421,36576,37572,74148,83273,52.9\n",
      "2023,10,70166,32611,20719,53330,16836,23.99\n",
      "2023,11,84035,27319,44342,71661,12374,14.72\n",
      "2023,12,58132,43394,30877,74271,-16139,-27.76\n",
      "2024,1,142246,69027,25365,94392,47854,33.64\n",
      "2024,2,66635,20495,40398,60893,5742,8.62\n",
      "2024,3,88465,28615,27045,55660,32805,37.08\n",
      "2024,4,114307,50143,39712,89855,24452,21.39\n",
      "2024,5,91673,64695,38848,103543,-11870,-12.95\n",
      "2024,6,187914,42640,31659,74299,113615,60.46\n",
      "2024,7,65116,33659,17439,51098,14018,21.53\n",
      "2024,8,162874,49547,12053,61600,101274,62.18\n",
      "2024,9,141970,55399,20012,75411,66559,46.88\n",
      "2024,10,76936,33380,29315,62695,14241,18.51\n",
      "2024,11,195670,68808,38278,107086,88584,45.27\n",
      "2024,12,109018,41704,35981,77685,31333,28.74\n",
      "2025,1,57122,60867,30718,91585,-34463,-60.33\n",
      "2025,2,87991,27800,37600,65400,22591,25.67\n",
      "2025,3,103727,40473,11286,51759,51968,50.1\n",
      "2025,4,77373,40938,38085,79023,-1650,-2.13\n",
      "2025,5,157262,29541,35906,65447,91815,58.38\n",
      "2025,6,178504,45652,27572,73224,105280,58.98\n",
      "2025,7,156258,37908,13922,51830,104428,66.83\n",
      "2025,8,99523,51705,41979,93684,5839,5.87\n",
      "2025,9,184895,21543,26991,48534,136361,73.75\n",
      "2025,10,186011,52438,35332,87770,98241,52.81\n",
      "2025,11,162253,56081,20376,76457,85796,52.88\n",
      "2025,12,147539,46748,28050,74798,72741,49.3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== Human Message ===============================\n",
      "My overall cosy in year 2024 compared to year 2025, 2021, 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 192, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 868, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 293, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 153, in retry_target\n",
      "    _retry_error_helper(\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 212, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 144, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.InvalidArgument: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\blocks.py\", line 2096, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\blocks.py\", line 1655, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\utils.py\", line 840, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\chat_interface.py\", line 899, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\utils.py\", line 735, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\utils.py\", line 729, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\gradio\\utils.py\", line 712, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_3404\\3549844492.py\", line 114, in predict\n",
      "    output = app.invoke({\"messages\": input_messages}, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2124, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1779, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 230, in tick\n",
      "    run_with_retry(\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 546, in invoke\n",
      "    input = step.invoke(input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 310, in invoke\n",
      "    ret = context.run(self.func, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_3404\\3549844492.py\", line 96, in call_model\n",
      "    response = llm.invoke(prompt)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 1199, in invoke\n",
      "    return super().invoke(input, config, stop=stop, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 371, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 956, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 775, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1021, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 1275, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 210, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n",
      "    return self(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\python\\Lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\python\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 204, in _chat_with_retry\n",
      "    raise ChatGoogleGenerativeAIError(\n",
      "langchain_google_genai.chat_models.ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      ", locale: \"en-US\"\n",
      "message: \"API key not valid. Please pass a valid API key.\"\n",
      "]\n",
      "During task with name 'model' and id 'd3e5523f-43c8-67bd-9875-0c023f420856'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "import gradio as gr\n",
    "\n",
    "##Load Data-------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd  \n",
    "def xlsx_to_string(file_path):  \n",
    "    try:  \n",
    "        df = pd.read_csv(file_path)  \n",
    "        content = df.to_csv(index=False)  \n",
    "        return content  \n",
    "    except Exception as e:  \n",
    "        print(\"Error: \", str(e))  \n",
    "# Usage example  \n",
    "file_path = r\"C:\\Users\\Acer\\Downloads\\16_Clariant\\sample-data.csv\"\n",
    "content_string = xlsx_to_string(file_path)  \n",
    "# print(content_string)  \n",
    "\n",
    "\n",
    "##Template -------------------------------------------------------------------------------------------------------\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "query=f\"\"\"You are a financial expert with years of experience, deeply analyze the given data. Which contains valuable financial information about a company's performance, and then provide with comprehensive answers to user questions based on your thorough analysis.\n",
    "Rules 1. Please avoid creating charts or graphs, as well as writing any code. Provide a simple and direct response using natural language without mentioning any limitations or rules.\n",
    "{content_string}\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            query,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "print(query)\n",
    "##Code Execution-------------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "def execute_code(MATPLOTLIB_CODE):\n",
    "    python_repl = PythonREPL()\n",
    "    return python_repl.run(MATPLOTLIB_CODE)\n",
    "\n",
    "\n",
    "##LLM Call for Generating Matplotlib Code-------------------------------------------------------------------------------------------------------\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "def llm_call_for_matplotlib(user_query, response):\n",
    "    try:\n",
    "        class Json_Parser(BaseModel):\n",
    "            MATPLOTLIB_CODE: str = Field(description=\"Matplotlb Code\")\n",
    "\n",
    "        matplotlib_query = f\"Provided a LLM response and a user query, write a matplotlib code for either a bar chart, scatter chart, a histogram, or line chart based on the given information. If it is not possible to create a graph, no code will be generated.\"\n",
    "        matplotlib_query += f\"\\n\\nUser query: {user_query}\"\n",
    "        matplotlib_query += f\"\\nLLM response: {response}\"\n",
    "        \n",
    "        print(matplotlib_query)\n",
    "        \n",
    "        parser = JsonOutputParser(pydantic_object=Json_Parser)\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "            input_variables=[\"query\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "        )\n",
    "\n",
    "        chain = prompt | llm | parser\n",
    "        json_result = chain.invoke({\"query\": matplotlib_query})\n",
    "        print(json_result['MATPLOTLIB_CODE'])\n",
    "        return json_result['MATPLOTLIB_CODE']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# user_query = \"In Which year was my sales maximum.\"\n",
    "# response = \"\"\" \n",
    "# To determine the year with the maximum sales (revenue), we need to sum the revenue for each year and compare the totals.\n",
    "# Here are the total revenues for each year:\n",
    "# - 2020: 1,748,215\n",
    "# - 2021: 1,548,561\n",
    "# - 2022: 1,678,708\n",
    "# - 2023: 1,451,712 (up to December)\n",
    "# - 2024: 1,548,140 (up to December)\n",
    "# - 2025: 1,548,158 (up to December)\n",
    "# From the data provided, the year 2020 had the maximum sales with a total revenue of 1,748,215.\n",
    "# \"\"\"\n",
    "# MATPLOTLIB_CODE = llm_call_for_matplotlib(user_query, response)\n",
    "# execute_code(MATPLOTLIB_CODE)\n",
    "\n",
    "\n",
    "\n",
    "##Chat bot-------------------------------------------------------------------------------------------------------\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "##Gradio-------------------------------------------------------------------------------------------------------\n",
    "import time\n",
    "def predict(message, history):\n",
    "    input_messages = [HumanMessage(message)]\n",
    "    print(\"================================== Human Message ===============================\")\n",
    "    print(message)\n",
    "    output = app.invoke({\"messages\": input_messages}, config)\n",
    "    print(\"================================== Ai Message ==================================\")\n",
    "    print(output[\"messages\"][-1].content)\n",
    "    result = output[\"messages\"][-1].content\n",
    "    print(\"================================== Matplotlib =================================\")\n",
    "    MATPLOTLIB_CODE = llm_call_for_matplotlib(message, result)\n",
    "    python_repl = PythonREPL()\n",
    "    python_repl.run(MATPLOTLIB_CODE)\n",
    "    Matplotlib_charts = execute_code(MATPLOTLIB_CODE)\n",
    "    print(Matplotlib_charts)\n",
    "    for i in range(len(result)):\n",
    "        yield result[: i+1]\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    predict,\n",
    "    type=\"messages\",\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fbe58ca63fe33f9eeae9e71d10368d2b4a57f2b1b395836210cc60d362c66949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
